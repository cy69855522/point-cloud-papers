# [Modeling Point Clouds with Self-Attention and Gumbel Subset Sampling](http://arxiv.org/pdf/1904.03375v1.pdf)
由于3D传感器的普及，几何深度学习变得越来越重要。受NLP域的最新进展的启发，引入自注意变换器以消耗点云。我们开发了点注意变压器（PAT），使用参数高效的组合注意力（GSA）来代替昂贵的多头注意力。我们展示了它处理大小变化输入的能力，并证明了它的置换等价性。此外，先前的工作使用启发式依赖于输入数据（例如，最远点采样）来分层地选择输入点的子集。因此，我们首次提出了一种名为Gumbel子集采样（GSS）的端到端可学习和任务相关的采样操作，以选择输入点的代表性子集。配备Gumbel-Softmax，它在训练阶段产生“软”连续子集，在测试阶段产生“硬”离散子集。通过以分层方式选择代表性子集，网络以较低的计算成本学习输入集的更强表示。分类实验和分类基准测试表明了我们方法的有效性和有效性。此外，我们提出了一种新颖的应用，将事件相机流处理为点云，并在DVS128手势数据集上实现最佳性能。

# 论文动机
- 引入自注意变换器
- 可微子集采样

# 模型流程
## ARPE 绝对相对位置嵌入
![](公式1.png)

![](公式2.png)
- 对于每个点 p，获得一组 K 近邻集合｛xp，xi-xp for i in K｝
- 使用 PointNet(ELU激活) 提取区域集合信息作为 p 点的特征
## GSA 与 GSS
![](采样.png)
### GSA 自注意变换器
![](公式3.png)
- S(Q,X) = softmax(QX^T/√c), 此处 √c 用于控制内积尺度，避免 softmax 非 1 即 0
- σ 为 ELU 函数

![](公式4.png)
- 分组式高效多头自注意力
- 参考 MobileNet 的 depth-wise，将 x 按通道分为多组，每组组内共享一个线性变换矩阵 W，然后通过自注意力机制获得同通道数的变换后的 wx（注意此处是wx，相较于x，其通道意义已经被改变）

![](公式5.png)
- 将多组通道拼接

![](公式6.png)
- 按规则 shuffle

![](公式7.png)
- 与原 x 进行快捷连接
- 通过 GN（group normalization） 调整分布
### GSS Gumbel子集采样
![](公式8.png)
- 受 Attention-based MIL pooling 启发，作者使用类似手法对点集进行可微池化
- 此处使用 gumbel 是因为作者认为直接使用 softmax 的池化是**不可追溯的**和**难以解释的**
- 相当于在点上做 N(i+1) 次选择，下标代表层数
## 前向传播
![](模型.png)

# 实验结果
## 语义分割

# 改进方向
- 1、
  - 
# 疑问
- 

# 参考
- 
